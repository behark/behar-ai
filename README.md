# Behar AI - Open WebUI Deployment

A production-ready Open WebUI setup with multiple LLM support.

## ğŸš€ Quick Deploy

### Railway (Recommended)
[![Deploy on Railway](https://railway.app/button.svg)](https://railway.app/template)

```bash
# Clone and deploy
git clone https://github.com/behark/behar-ai.git
cd behar-ai
railway login
railway init
railway up
```

### Docker (Self-hosted)
```bash
# Clone repository
git clone https://github.com/behark/behar-ai.git
cd behar-ai

# Build and run
docker build -f Dockerfile.deploy -t behar-ai .
docker run -p 8080:8080 behar-ai
```

### VPS Deployment
```bash
# On your VPS
git clone https://github.com/behark/behar-ai.git
cd behar-ai
cp .env.deploy .env
# Edit .env with your settings
nano .env

# Run with Docker Compose
docker-compose up -d
```

## ğŸ› ï¸ Configuration

1. **Copy environment template:**
   ```bash
   cp .env.deploy .env
   ```

2. **Edit configuration:**
   - Set `WEBUI_SECRET_KEY` to a secure random string
   - Configure `OLLAMA_BASE_URL` if using external Ollama
   - Set `CORS_ALLOW_ORIGIN` to your domain

3. **Deploy and access:**
   - Railway: Automatic HTTPS URL provided
   - VPS: Configure nginx + SSL
   - Local: http://localhost:8080

## ğŸ”’ Security

- No personal data in repository
- Environment variables for secrets
- CORS protection
- Rate limiting enabled
- Authentication required

## ğŸ“Š Features

- Multiple LLM support via Ollama
- Clean, responsive web interface
- API access for integrations
- Real-time chat streaming
- Model switching
- Chat history

## ğŸ†˜ Support

For issues or questions, please open an issue in this repository.
